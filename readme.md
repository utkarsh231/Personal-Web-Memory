# Personal Web Memory - End-to-End Demo

A lightweight system that turns your browsing history into a searchable personal memory layer.
This project demonstrates the full workflow Sid requested: capture -> process -> index -> retrieve -> agent -> UI.

Built using Python, BeautifulSoup, Chroma, LangChain, OpenAI Embeddings, and Streamlit.

--------

## What This Project Does

This system takes raw browsing behavior (Chrome history), processes it, builds a vector index, and exposes an intelligent agent that can answer questions such as:
	â€¢	â€œWhat product pages did I look at?â€
	â€¢	â€œWhich pages did I visit about AirPods?â€
	â€¢	â€œShow me the page I looked at about agents this week.â€

It includes:
	1.	History extraction
	2.	Web page scraping + fallback logic
	3.	Embedding + vector store indexing
	4.	A retrieval-grounded QA agent with a custom prompt
	5.	A simple Streamlit UI for running queries

Everything runs locally and persists to ./data/chroma_db.

------

## Project Architecture

Chrome History
      â†“
scrape_pages.py  â€” extract + clean content (title fallback)
      â†“
build_index.py   â€” embed & store in Chroma
      â†“
agent.py         â€” RetrievalQA agent over vector store
      â†“
streamlit_app.py â€” simple UI for querying


-----

### Repository Structure

personal-web-memory/
â”‚
â”œâ”€â”€ extract_history.py
â”œâ”€â”€ scrape_pages.py
â”œâ”€â”€ build_index.py
â”œâ”€â”€ agent.py
â”œâ”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ scraped_pages.jsonl
    â”œâ”€â”€ profile.json           (optional)
    â””â”€â”€ chroma_db/             (auto-created)


â¸»

ğŸ›  Setup Instructions

1. Create virtual environment

python3 -m venv .venv
source .venv/bin/activate

2. Install dependencies

pip install --upgrade pip
pip install -r requirements.txt

3. Add your OpenAI API key

Create .env or export manually:

export OPENAI_API_KEY=your_key_here


-----

## Extract Your Chrome History

python extract_history.py

Produces:

data/chrome_history.csv


----

## Scrape Visited Pages

python scrape_pages.py

Produces:

data/scraped_pages.jsonl

Includes fallback logic: if pages block scraping and return empty content, the system uses the page title + URL so every page still contributes meaningfully to the memory.

----

ğŸ“š 3. Build the Vector Index

python build_index.py

This:
	â€¢	Embeds each page using OpenAIâ€™s text-embedding-3-small
	â€¢	Stores them inside a persistent Chroma DB
	â€¢	Prints how many documents were indexed (useful sanity check)

Generates:

data/chroma_db/


------

## 4. Run the Personal Web Memory Agent

python agent.py

This loads the index and answers a sample query.
The agent uses a custom retrieval-grounded prompt:
	â€¢	It must answer only using retrieved context
	â€¢	If nothing is relevant, it returns:
â€œI couldnâ€™t find that in your browsing history.â€

-----

## 5. Run the Streamlit App

streamlit run streamlit_app.py

Open your browser at:

http://localhost:8501

You will see a single input box.
Ask questions like:

âœ… Example queries
	â€¢	â€œWhat product pages did I visit recently?â€
	â€¢	â€œShow me the pages I viewed about AirPods.â€
	â€¢	â€œWhat pages did I read about watches?â€
	â€¢	â€œWhich shopping sites did I open this week?â€
	â€¢	â€œSummarize the product pages I looked at.â€

-----

## Agent Prompting Logic

The RetrievalQA agent is intentionally simple:
	â€¢	Query â†’ embed â†’ retrieve top-k documents
	â€¢	Agent answers using only retrieved text
	â€¢	Encourages grounding, reduces hallucination
	â€¢	Fallback line when there is truly no match

This models the early version of what Noraâ€™s personal-shopping memory layer would do.

-----

## Debugging Tools Included
	â€¢	build_index.py prints how many documents were added
	â€¢	agent.py (optional mode) prints source documents used
	â€¢	Scraper includes error handling for blocked websites
	â€¢	Index rebuild wipes old DB to avoid stale embeddings


